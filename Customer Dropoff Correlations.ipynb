{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          CUSTOMER_KEY  CUSTOMER_ACTIVITY_KEY LINE_OF_BUSINESS  \\\n",
      "0 -8061130536551660000   -9223361993734550000              NaN   \n",
      "1 -2653017699136930000   -9223341076556800000              NaN   \n",
      "2  1013516401290274270   -9223333229117640000              NaN   \n",
      "3 -7596488359783870000   -9223325886369360000              NaN   \n",
      "4 -2234331647526360000   -9223324519101500000              NaN   \n",
      "\n",
      "  ACTIVITY_SOURCE ACTIVITY_CODE  ACTIVITY_TYPE        ACTIVITY_DATETIME  \\\n",
      "0          eloqua    email_open  communication  2017-04-25 18:00:52.357   \n",
      "1          eloqua    email_send  communication  2024-01-08 00:06:59.290   \n",
      "2          eloqua    email_send  communication  2020-08-25 21:02:16.683   \n",
      "3          eloqua    email_send  communication  2017-11-03 14:14:06.690   \n",
      "4          eloqua    email_open  communication  2021-12-04 11:46:56.013   \n",
      "\n",
      "   IS_ENGAGEMENT_ACTIVITY customer_status  \n",
      "0                       1            lost  \n",
      "1                       0            lost  \n",
      "2                       0            lost  \n",
      "3                       0            lost  \n",
      "4                       1            lost  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths for lost list CSVs\n",
    "file_paths = [\n",
    "    r\"C:\\Users\\cdub4\\OneDrive\\Documentos\\Seahawks\\lost_stnl_and_bp_activity_subset_1.csv\",\n",
    "    r\"C:\\Users\\cdub4\\OneDrive\\Documentos\\Seahawks\\lost_stnl_and_bp_activity_subset_2.csv\",\n",
    "    r\"C:\\Users\\cdub4\\OneDrive\\Documentos\\Seahawks\\lost_stnl_and_bp_activity_subset_3.csv\",\n",
    "    r\"C:\\Users\\cdub4\\OneDrive\\Documentos\\Seahawks\\lost_stnl_and_bp_activity_subset_4.csv\",\n",
    "    r\"C:\\Users\\cdub4\\OneDrive\\Documentos\\Seahawks\\lost_stnl_and_bp_activity_subset_5.csv\"\n",
    "]\n",
    "\n",
    "# Read and combine lost CSV files\n",
    "lost_dfs = [pd.read_csv(file) for file in file_paths]\n",
    "combined_lost_df = pd.concat(lost_dfs, ignore_index=True)\n",
    "\n",
    "# Filter rows where CAMPAIGN_LOST is \"STNL to BP\" and add a status column\n",
    "lost_df = combined_lost_df[combined_lost_df['CAMPAIGN_LOST'] == \"STNL to BP\"].copy()\n",
    "lost_df['customer_status'] = 'lost'\n",
    "\n",
    "# Read BP list CSV and add a status column\n",
    "bp_df = pd.read_csv(r\"C:\\Users\\cdub4\\OneDrive\\Documentos\\Seahawks\\bp_activity_subset.csv\")\n",
    "bp_df['customer_status'] = 'current'\n",
    "\n",
    "# Keep only the columns common to both DataFrames\n",
    "common_cols = lost_df.columns.intersection(bp_df.columns)\n",
    "lost_df = lost_df[common_cols]\n",
    "bp_df = bp_df[common_cols]\n",
    "\n",
    "# Combine the two DataFrames\n",
    "combined_customers = pd.concat([lost_df, bp_df], ignore_index=True)\n",
    "\n",
    "# Optional: Save combined DataFrame to CSV\n",
    "combined_customers.to_csv(r\"C:\\Users\\cdub4\\OneDrive\\Documentos\\Seahawks\\combined_customer_list.csv\", index=False)\n",
    "\n",
    "print(combined_customers.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer_status\n",
      "lost       4006493\n",
      "current    1116084\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(combined_customers['customer_status'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           CUSTOMER_KEY    last_engagement_date  days_since_last_engagement  \\\n",
      "0  -9222442665376300000 2024-12-10 18:25:11.160                          54   \n",
      "1  -9219762487645540000 2023-05-09 19:08:53.440                         635   \n",
      "2  -9219156172471900000 2022-08-01 12:29:51.188                         917   \n",
      "3  -9219114118600400000 2025-01-10 18:06:16.247                          23   \n",
      "4  -9217062962888802692 2024-10-14 07:00:40.000                         112   \n",
      "..                  ...                     ...                         ...   \n",
      "95 -9040186546145990000 2022-08-01 12:30:10.830                         917   \n",
      "96 -9039277127145240000 2024-11-20 21:27:05.103                          74   \n",
      "97 -9039198197510877527 2024-12-10 14:56:44.390                          54   \n",
      "98 -9038257846757470000 2024-08-30 18:16:14.561                         156   \n",
      "99 -9036987208130000000 2023-06-27 14:26:53.685                         586   \n",
      "\n",
      "    game_purchase_count  merch_purchase_count  resell_purchase_count  \\\n",
      "0                   1.0                   0.0                    2.0   \n",
      "1                   0.0                   0.0                    0.0   \n",
      "2                   0.0                   0.0                    0.0   \n",
      "3                   2.0                   0.0                    0.0   \n",
      "4                   2.0                   0.0                    0.0   \n",
      "..                  ...                   ...                    ...   \n",
      "95                  0.0                   0.0                    0.0   \n",
      "96                  5.0                   1.0                    1.0   \n",
      "97                  1.0                   0.0                    1.0   \n",
      "98                  2.0                   0.0                    1.0   \n",
      "99                  0.0                   0.0                    0.0   \n",
      "\n",
      "   customer_status  \n",
      "0             lost  \n",
      "1             lost  \n",
      "2             lost  \n",
      "3             lost  \n",
      "4          current  \n",
      "..             ...  \n",
      "95            lost  \n",
      "96            lost  \n",
      "97         current  \n",
      "98            lost  \n",
      "99            lost  \n",
      "\n",
      "[100 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set reference date\n",
    "reference_date = pd.Timestamp('now')\n",
    "\n",
    "# Ensure ACTIVITY_DATETIME is datetime\n",
    "combined_customers['ACTIVITY_DATETIME'] = pd.to_datetime(combined_customers['ACTIVITY_DATETIME'], errors='coerce')\n",
    "\n",
    "# Derive last engagement date and days since last engagement\n",
    "engagement_df = combined_customers[combined_customers['IS_ENGAGEMENT_ACTIVITY'] == 1]\n",
    "last_engagement = engagement_df.groupby('CUSTOMER_KEY')['ACTIVITY_DATETIME'].max().reset_index().rename(columns={'ACTIVITY_DATETIME': 'last_engagement_date'})\n",
    "last_engagement['days_since_last_engagement'] = (reference_date - last_engagement['last_engagement_date']).dt.days\n",
    "\n",
    "# Define purchase codes\n",
    "game_purchase_types = [\"primary_purchase_club\", \"primary_purchase_ga\", \"primary_purchase_single\", \"primary_purchase_sth\", \"primary_purchase_suite\"]\n",
    "resell_purchase_types = [\"secondary_purchase\"]\n",
    "merch_purchase_types = [\"merch_purchase\"]\n",
    "purchase_codes = game_purchase_types + resell_purchase_types + merch_purchase_types\n",
    "\n",
    "# Filter purchase events and compute counts\n",
    "purchase_df = combined_customers[(combined_customers['ACTIVITY_TYPE'] == 'purchase') & (combined_customers['ACTIVITY_CODE'].isin(purchase_codes))]\n",
    "\n",
    "game_counts = purchase_df[purchase_df['ACTIVITY_CODE'].isin(game_purchase_types)].groupby('CUSTOMER_KEY').size().reset_index(name='game_purchase_count')\n",
    "merch_counts = purchase_df[purchase_df['ACTIVITY_CODE'].isin(merch_purchase_types)].groupby('CUSTOMER_KEY').size().reset_index(name='merch_purchase_count')\n",
    "resell_counts = purchase_df[purchase_df['ACTIVITY_CODE'].isin(resell_purchase_types)].groupby('CUSTOMER_KEY').size().reset_index(name='resell_purchase_count')\n",
    "\n",
    "# Merge counts with last engagement info\n",
    "customer_summary = last_engagement.copy()\n",
    "customer_summary = customer_summary.merge(game_counts, on='CUSTOMER_KEY', how='left')\n",
    "customer_summary = customer_summary.merge(merch_counts, on='CUSTOMER_KEY', how='left')\n",
    "customer_summary = customer_summary.merge(resell_counts, on='CUSTOMER_KEY', how='left')\n",
    "\n",
    "# Fill NaN counts with 0\n",
    "for col in ['game_purchase_count', 'merch_purchase_count', 'resell_purchase_count']:\n",
    "    customer_summary[col] = customer_summary[col].fillna(0)\n",
    "\n",
    "# Merge customer_status if available\n",
    "customer_status = combined_customers[['CUSTOMER_KEY','customer_status']].drop_duplicates()\n",
    "customer_summary = customer_summary.merge(customer_status, on='CUSTOMER_KEY', how='left')\n",
    "\n",
    "print(customer_summary.head(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                days_since_last_engagement                     \\\n",
      "                                      mean median         std   \n",
      "customer_status                                                 \n",
      "current                         140.704667   89.0  140.746727   \n",
      "lost                            450.110385  234.0  502.851731   \n",
      "\n",
      "                game_purchase_count                  merch_purchase_count  \\\n",
      "                               mean median       std                 mean   \n",
      "customer_status                                                             \n",
      "current                    1.842000    1.0  2.027752             0.787333   \n",
      "lost                       0.332057    0.0  1.571728             0.314260   \n",
      "\n",
      "                                 resell_purchase_count                   \n",
      "                median       std                  mean median       std  \n",
      "customer_status                                                          \n",
      "current            0.0  2.513361              0.569333    0.0  1.480987  \n",
      "lost               0.0  1.604222              0.170083    0.0  0.793063  \n"
     ]
    }
   ],
   "source": [
    "print(customer_summary.groupby('customer_status').agg({\n",
    "    'days_since_last_engagement': ['mean', 'median', 'std'],\n",
    "    'game_purchase_count': ['mean', 'median', 'std'],\n",
    "    'merch_purchase_count': ['mean', 'median', 'std'],\n",
    "    'resell_purchase_count': ['mean', 'median', 'std']\n",
    "}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            days_since_last_engagement  game_purchase_count  \\\n",
      "days_since_last_engagement                    1.000000            -0.201462   \n",
      "game_purchase_count                          -0.201462             1.000000   \n",
      "merch_purchase_count                         -0.114527             0.159804   \n",
      "resell_purchase_count                        -0.131462             0.184114   \n",
      "\n",
      "                            merch_purchase_count  resell_purchase_count  \n",
      "days_since_last_engagement             -0.114527              -0.131462  \n",
      "game_purchase_count                     0.159804               0.184114  \n",
      "merch_purchase_count                    1.000000               0.126472  \n",
      "resell_purchase_count                   0.126472               1.000000  \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.07      0.12       448\n",
      "           1       0.86      0.97      0.91      2666\n",
      "\n",
      "    accuracy                           0.84      3114\n",
      "   macro avg       0.59      0.52      0.52      3114\n",
      "weighted avg       0.78      0.84      0.80      3114\n",
      "\n",
      "Model coefficients: {'days_since_last_engagement': 0.0026408419001815067, 'game_purchase_count': -0.4413431451496819, 'merch_purchase_count': -0.010189807517084334, 'resell_purchase_count': -0.11292611910936305}\n"
     ]
    }
   ],
   "source": [
    "# Compute correlation matrix\n",
    "corr_cols = ['days_since_last_engagement', 'game_purchase_count', 'merch_purchase_count', 'resell_purchase_count']\n",
    "print(customer_summary[corr_cols].corr())\n",
    "\n",
    "# Prepare data for logistic regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Create a binary target: lost = 1, current = 0\n",
    "customer_summary['lost_flag'] = (customer_summary['customer_status'] == 'lost').astype(int)\n",
    "\n",
    "# Fill missing values if necessary\n",
    "X = customer_summary[corr_cols].fillna(0)\n",
    "y = customer_summary['lost_flag']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Model coefficients:\", dict(zip(corr_cols, model.coef_[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "days_since_last_engagement    0.226373\n",
      "game_purchase_count          -0.307113\n",
      "merch_purchase_count         -0.093855\n",
      "resell_purchase_count        -0.150126\n",
      "Name: lost_flag, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate correlations between lost_flag and the four variables\n",
    "cols = ['days_since_last_engagement', 'game_purchase_count', 'merch_purchase_count', 'resell_purchase_count']\n",
    "correlations = customer_summary[cols + ['lost_flag']].corr()['lost_flag'][cols]\n",
    "print(correlations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "days_since_last_engagement    0.226373\n",
      "game_purchase_count          -0.307113\n",
      "merch_purchase_count         -0.093855\n",
      "resell_purchase_count        -0.150126\n",
      "email_count                  -0.123920\n",
      "Name: lost_flag, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate email count per CUSTOMER_KEY where ACTIVITY_CODE is 'email_send'\n",
    "email_counts = combined_customers[combined_customers['ACTIVITY_CODE'] == 'email_send'].groupby('CUSTOMER_KEY').size().reset_index(name='email_count')\n",
    "\n",
    "# Merge the email counts with customer_summary and fill missing values with 0\n",
    "customer_summary = customer_summary.merge(email_counts, on='CUSTOMER_KEY', how='left')\n",
    "customer_summary['email_count'] = customer_summary['email_count'].fillna(0)\n",
    "\n",
    "# Calculate correlations between lost_flag and the 5 variables including email_count\n",
    "cols = ['days_since_last_engagement', 'game_purchase_count', 'merch_purchase_count', 'resell_purchase_count', 'email_count']\n",
    "correlations = customer_summary[cols + ['lost_flag']].corr()['lost_flag'][cols]\n",
    "print(correlations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, game purchase count is most correlated with customer dropoff from the STNL to BP list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "days_since_last_engagement    0.312465\n",
      "game_purchase_count          -0.179496\n",
      "merch_purchase_count         -0.040606\n",
      "resell_purchase_count        -0.020248\n",
      "email_count                  -0.022551\n",
      "Name: lost_flag, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "combined_lost_df = pd.concat(lost_dfs, ignore_index=True)\n",
    "\n",
    "# Filter lost list for CAMPAIGN_LOST == \"BP to STH\" and add a status column\n",
    "lost_df = combined_lost_df[combined_lost_df['CAMPAIGN_LOST'] == \"BP to STH\"].copy()\n",
    "lost_df['customer_status'] = 'lost'\n",
    "\n",
    "# Read current STH dataset\n",
    "sth_df = pd.read_csv(r\"C:/Users/cdub4/OneDrive/Documentos/Seahawks/new_sth_subset.csv\")\n",
    "sth_df['customer_status'] = 'current'\n",
    "\n",
    "# Keep only common columns and combine the datasets\n",
    "common_cols = lost_df.columns.intersection(sth_df.columns)\n",
    "lost_df = lost_df[common_cols]\n",
    "sth_df = sth_df[common_cols]\n",
    "combined_customers = pd.concat([lost_df, sth_df], ignore_index=True)\n",
    "\n",
    "# Ensure ACTIVITY_DATETIME is datetime\n",
    "combined_customers['ACTIVITY_DATETIME'] = pd.to_datetime(combined_customers['ACTIVITY_DATETIME'], errors='coerce')\n",
    "reference_date = pd.Timestamp('now')\n",
    "\n",
    "# Derive last engagement date and compute days_since_last_engagement\n",
    "engagement_df = combined_customers[combined_customers['IS_ENGAGEMENT_ACTIVITY'] == 1]\n",
    "last_engagement = engagement_df.groupby('CUSTOMER_KEY')['ACTIVITY_DATETIME'].max().reset_index().rename(columns={'ACTIVITY_DATETIME': 'last_engagement_date'})\n",
    "last_engagement['days_since_last_engagement'] = (reference_date - last_engagement['last_engagement_date']).dt.days\n",
    "\n",
    "# Define purchase codes\n",
    "game_purchase_types = [\"primary_purchase_club\", \"primary_purchase_ga\", \"primary_purchase_single\", \"primary_purchase_sth\", \"primary_purchase_suite\"]\n",
    "resell_purchase_types = [\"secondary_purchase\"]\n",
    "merch_purchase_types = [\"merch_purchase\"]\n",
    "purchase_codes = game_purchase_types + resell_purchase_types + merch_purchase_types\n",
    "\n",
    "# Filter purchase events and compute purchase counts\n",
    "purchase_df = combined_customers[(combined_customers['ACTIVITY_TYPE'] == 'purchase') & (combined_customers['ACTIVITY_CODE'].isin(purchase_codes))]\n",
    "game_counts = purchase_df[purchase_df['ACTIVITY_CODE'].isin(game_purchase_types)].groupby('CUSTOMER_KEY').size().reset_index(name='game_purchase_count')\n",
    "merch_counts = purchase_df[purchase_df['ACTIVITY_CODE'].isin(merch_purchase_types)].groupby('CUSTOMER_KEY').size().reset_index(name='merch_purchase_count')\n",
    "resell_counts = purchase_df[purchase_df['ACTIVITY_CODE'].isin(resell_purchase_types)].groupby('CUSTOMER_KEY').size().reset_index(name='resell_purchase_count')\n",
    "\n",
    "# Calculate email counts\n",
    "email_counts = combined_customers[combined_customers['ACTIVITY_CODE'] == 'email_send'].groupby('CUSTOMER_KEY').size().reset_index(name='email_count')\n",
    "\n",
    "# Merge all metrics into a customer_summary\n",
    "customer_summary = last_engagement.copy()\n",
    "customer_summary = customer_summary.merge(game_counts, on='CUSTOMER_KEY', how='left')\n",
    "customer_summary = customer_summary.merge(merch_counts, on='CUSTOMER_KEY', how='left')\n",
    "customer_summary = customer_summary.merge(resell_counts, on='CUSTOMER_KEY', how='left')\n",
    "customer_summary = customer_summary.merge(email_counts, on='CUSTOMER_KEY', how='left')\n",
    "\n",
    "# Fill missing counts with 0\n",
    "for col in ['game_purchase_count', 'merch_purchase_count', 'resell_purchase_count', 'email_count']:\n",
    "    customer_summary[col] = customer_summary[col].fillna(0)\n",
    "\n",
    "# Merge customer_status\n",
    "status_df = combined_customers[['CUSTOMER_KEY', 'customer_status']].drop_duplicates()\n",
    "customer_summary = customer_summary.merge(status_df, on='CUSTOMER_KEY', how='left')\n",
    "\n",
    "# Create binary flag for lost (lost = 1, current = 0)\n",
    "customer_summary['lost_flag'] = (customer_summary['customer_status'] == 'lost').astype(int)\n",
    "\n",
    "# Compute correlations between lost_flag and selected variables\n",
    "cols = ['days_since_last_engagement', 'game_purchase_count', 'merch_purchase_count', 'resell_purchase_count', 'email_count']\n",
    "correlations = customer_summary[cols + ['lost_flag']].corr()['lost_flag'][cols]\n",
    "print(correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, days since last engagement has the highest correlation with customer dropoff from the BP to STH list."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
